# 分布式一致性

zab：https://houbb.github.io/2018/10/30/zab

2pc：https://zh.wikipedia.org/zh-hans/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%88%E6%8F%90%E4%BA%A4%E8%AF%B7%E6%B1%82%EF%BC%89

paxos：https://pdai.tech/md/algorithm/alg-domain-distribute-x-paxos.html#basic-paxos%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0

raft：https://pdai.tech/md/algorithm/alg-domain-distribute-x-raft.html



## paxos

paxos算法运行在服务器发生宕机故障的时候，能够保证数据的完整性，不要求可靠的消息传递，可容忍消息丢失，延迟，乱序以及重复，保证服务高可用。

要解决的问题是：一个分布式系统的各个进程如何就某一个值（决议）达成一致。

一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。

### 角色

Proposer 提议这

Accepter 决策者

Learner 最终决策学习者

`Proposer`: 提出提案 (Proposal)。Proposal信息包括提案编号 (Proposal ID) 和提议的值 (Value)。

`Acceptor`: 参与决策，回应Proposers的提案。收到Proposal后可以接受提案，若Proposal获得多数Acceptors的接受，则称该Proposal被批准。

`Learner`: 不参与决策，从Proposers/Acceptors学习最新达成一致的提案(Value)。

在多副本状态机中，每个副本同时具有Proposer、Acceptor、Learner三种角色。

![img](https://pdai.tech/images/alg/alg-dst-paxos-1.jpg)

### 第一阶段：Prepare

Proposer向Acceptors发出Prepare请求，Acceptors针对收到的Prepare请求进行Promise承诺。

- `Prepare`: Proposer生成全局唯一且递增的Proposal ID (可使用时间戳加Server ID)，向所有Acceptors发送Prepare请求，这里无需携带提案内容，只携带Proposal ID即可。
- `Promise`: Acceptors收到Prepare请求后，做出“两个承诺，一个应答”。
  - 承诺1: 不再接受Proposal ID小于等于(注意: 这里是<= )当前请求的Prepare请求;
  - 承诺2: 不再接受Proposal ID小于(注意: 这里是< )当前请求的Propose请求;
  - 应答: 不违背以前作出的承诺下，回复已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID，没有则返回空值。

### 第二阶段: Accept阶段

Proposer收到多数Acceptors承诺的Promise后，向Acceptors发出Propose请求，Acceptors针对收到的Propose请求进行Accept处理。

- `Propose`: Proposer 收到多数Acceptors的Promise应答后，从应答中选择Proposal ID最大的提案的Value，作为本次要发起的提案。如果所有应答的提案Value均为空值，则可以自己随意决定提案Value。然后携带当前Proposal ID，向所有Acceptors发送Propose请求。
- `Accept`: Acceptor收到Propose请求后，在不违背自己之前作出的承诺下，接受并持久化当前Proposal ID和提案Value。

### 第三阶段: Learn阶段

Proposer在收到多数Acceptors的Accept之后，标志着本次Accept成功，决议形成，将形成的决议发送给所有Learners。



## multi-paxos

原始的Paxos算法(Basic Paxos)只能对一个值形成决议，决议的形成至少需要两次网络来回，在高并发情况下可能需要更多的网络来回，极端情况下甚至可能形成活锁。如果想连续确定多个值，Basic Paxos搞不定了。因此Basic Paxos几乎只是用来做理论研究，并不直接应用在实际工程中。

针对每一个要确定的值，运行一次Paxos算法实例(Instance)，形成决议。每一个Paxos实例使用唯一的Instance ID标识。

在所有Proposers中选举一个Leader，由Leader唯一地提交Proposal给Acceptors进行表决。这样没有Proposer竞争，解决了活锁问题。在系统中仅有一个Leader进行Value提交的情况下，Prepare阶段就可以跳过，从而将两阶段变为一阶段，提高效率。

Multi-Paxos首先需要选举Leader，Leader的确定也是一次决议的形成，所以可执行一次Basic Paxos实例来选举出一个Leader。选出Leader之后只能由Leader提交Proposal，在Leader宕机之后服务临时不可用，需要重新选举Leader继续服务。在系统中仅有一个Leader进行Proposal提交的情况下，Prepare阶段可以跳过。



Multi-Paxos通过改变Prepare阶段的作用范围至后面Leader提交的所有实例，从而使得Leader的连续提交只需要执行一次Prepare阶段，后续只需要执行Accept阶段，将两阶段变为一阶段，提高了效率。为了区分连续提交的多个实例，每个实例使用一个Instance ID标识，Instance ID由Leader本地递增生成即可。

Multi-Paxos允许有多个自认为是Leader的节点并发提交Proposal而不影响其安全性，这样的场景即退化为Basic Paxos。

Chubby和Boxwood均使用Multi-Paxos。ZooKeeper使用的Zab也是Multi-Paxos的变形。

## zab和paxos算法的区别与联系

1. Basic Paxos算法中是没有leader这个角色的，可以有多个提出提案的Proposer，而在zab算法中会选择出一个（全局唯一的服务器）leader，由leader来接收客户端的写请求。
2. zab协议保证事务的有序：zxid是一个全局单调递增的id，能够确保事务按序提交，这是因为zookeeper是一个树形的结构，如果乱序，很多操作都将无法执行。比如P1的事务t1可能是创建节点”/a”，t2可能是创建节点”/a/bb”，只有先创建了父节点”/a”，才能创建子节点”/a/b”。为了保证这一点
   - 事务由leader发起，leader保证发起的有序性
   - 使用fifo队列保证有序性
3. 相同之处是，和2pc要求所有副本服务器响应不同，zab和paxos都是当集群中的过半服务器对该提案通过之后，就会执行接下来的处理。
4. zab是在multi-paxos的基础上改进得来。

## 两阶段提交

两阶段提交协议的目标在于为分布式系统保证数据的一致性，许多分布式系统采用该协议提供对分布式事务的支持。顾名思义，该协议将一个分布式的事务过程拆分成两个阶段： **投票** 和 **事务提交** 。为了让整个数据库集群能够正常的运行，该协议指定了一个 **协调者** 单点，用于协调整个数据库集群各节点的运行。为了简化描述，我们将数据库集群中的各个节点称为 **参与者** ，**三阶段提交协议中同样包含协调者和参与者这两个角色定义**。

### 第一阶段：投票

1. 协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果；
2. 事务参与者收到请求之后，执行事务但不提交，并记录事务日志；
3. 参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。

### 第二阶段：事务提交

在经过第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在 3 种可能性：

1. 所有的参与者都回复能够正常执行事务。
2. 一个或多个参与者回复事务执行失败。
3. 协调者等待超时。

对于第 1 种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下：

1. 协调者向各个参与者发送 commit 通知，请求提交事务；
2. 参与者收到事务提交通知之后执行 commit 操作，然后释放占有的资源；
3. 参与者向协调者返回事务 commit 结果信息。

对于第 2 和第 3 种情况，协调者均认为参与者无法成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下：

1. 协调者向各个参与者发送事务 rollback 通知，请求回滚事务；
2. 参与者收到事务回滚通知之后执行 rollback 操作，然后释放占有的资源；
3. 参与者向协调者返回事务 rollback 结果信息。



### 缺点

- 单点问题
- 同步阻塞，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率极其低下
- 数据不一致：两阶段提交协议虽然是分布式数据强一致性所设计，但仍然存在数据不一致性的可能性。比如在第二阶段中，假设协调者发出了事务 commit 通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。

## 三阶段提交

针对两阶段提交存在的问题，三阶段提交协议通过引入一个 **准备** 阶段，以及超时策略来减少整个集群的阻塞时间，提升系统性能。三阶段提交的三个阶段分别为：准备（can_commit）、预提交（pre_commit），以及事务提交（do_commit）。

与两阶段提交不同的是，三阶段提交是一个“非阻塞”的协议。三阶段提交在两阶段提交的第一阶段与第二阶段之间插入了一个准备阶段，令原先在两阶段提交中，参与者在投票之后，**由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题**得以解决。

### 第一阶段 准备

该阶段协调者会去询问各个参与者是否能够正常执行事务，参与者根据自身情况回复一个预估值，相对于真正的执行事务，这个过程是轻量的，具体步骤如下：

1. 协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待回复；
2. 各个参与者依据自身状况回复一个预估值，如果预估自己能够正常执行事务就返回确定信息，并进入预备状态，否则返回否定信息。

### 第二阶段 预提交

本阶段协调者会根据第一阶段的询盘结果采取相应操作，询盘结果主要有 3 种：

1. 所有的参与者都返回确定信息。
2. 一个或多个参与者返回否定信息。
3. 协调者等待超时。

针对第 1 种情况，协调者会向所有参与者发送事务执行请求，具体步骤如下：

1. 协调者向所有的事务参与者发送事务执行通知；
2. 参与者收到通知后执行事务但不提交；
3. 参与者将事务执行情况返回给客户端。

针对第 2 和第 3 种情况，协调者认为事务无法正常执行，于是向各个参与者发出 abort 通知，请求**退出预备状态**，具体步骤如下：

1. 协调者向所有事务参与者发送 abort 通知；
2. 参与者收到通知后中断事务。

另外**如果参与者等待超时，也会中断事务**

### 第三阶段  事务提交

如果第二阶段事务未中断，那么本阶段协调者将会依据事务执行返回的结果来决定提交或回滚事务，分为 3 种情况：

1. 所有的参与者都能正常执行事务。
2. 一个或多个参与者执行事务失败。
3. 协调者等待超时。

针对第 1 种情况，协调者向各个参与者发起事务提交请求，具体步骤如下：

1. 协调者向所有参与者发送事务 commit 通知；
2. 所有参与者在收到通知之后执行 commit 操作，并释放占有的资源；
3. 参与者向协调者反馈事务提交结果。



针对第 2 和第 3 种情况，协调者认为事务无法成功执行，于是向各个参与者发送事务回滚请求，具体步骤如下：

1. 协调者向所有参与者发送事务 rollback 通知；
2. 所有参与者在收到通知之后执行 rollback 操作，并释放占有的资源；
3. 参与者向协调者反馈事务回滚结果。

## 对比

1. 超时机制解决同步阻塞问题，但是无法完全避免数据的不一致

在本阶段如果因为协调者或网络问题，导致参与者迟迟不能收到来自协调者的 commit 或 rollback 请求，那么参与者将不会如两阶段提交中那样陷入阻塞，而是等待超时后继续 commit，相对于两阶段提交虽然降低了同步阻塞，但仍然无法完全避免数据的不一致。

2. 三阶段提交协议对数据强一致性更有保障，但是因为效率问题，两阶段提交协议在实际系统中反而更加受宠。