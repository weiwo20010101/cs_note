# 实现服务发现的方法

![img](https://pic2.zhimg.com/80/v2-c5e1d05128694eaffc043d1acf1cab41_1440w.webp)

## 客户端发现模式

在客户端模式下，如果要进行微服务调用，首先要进行的是到服务注册中心获取服务列表，然后再根据调用端本地的负载均衡策略，进行服务调用。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3f00a29f599e46a5b7886c7958ac291b~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

在上图中，client端提供了负载均衡的功能，其首先从注册中心获取服务提供者的列表，然后通过自身负载均衡算法，选择一个最合理的服务提供者进行调用:

1、 服务提供者向注册中心进行注册，提交自己的相关信息

2、 服务消费者定期从注册中心获取服务提供者列表

3、 服务消费者通过自身的负载均衡算法，在服务提供者列表里面选择一个合适的服务提供者，进行访问

客户端发现模式的优缺点如下：

- 优点：
  - 负载均衡作为client中一个功能，用自身的算法，从服务提供者列表中选择一个合适服务提供者进行访问，因此client端可以定制化负载均衡算法。优点是服务客户端可以灵活、智能地制定负载均衡策略，包括轮询、加权轮询、一致性哈希等策略。
  - 可以实现点对点的网状通讯，即去中心化的通讯。可以有效避开单点造成的性能瓶颈和可靠性下降等问题。
  - 服务客户端通常以SDK的方式直接引入到项目，这种方式语言的整合程度最佳，程序执行性能最佳，程序错误排查更加容易。
- 缺点：
  - 当负载均衡算法需要更新时候，很难做到同一时间全部更新，所以就造成新旧算法同时运行
  - 客户端与注册中心紧密耦合，如果要换注册中心，需要去修改代码，重新上线。微服务的规模越大，服务更新越困难，这在一定程度上违背了微服务架构提倡的技术独立性。

*目前来说，大部分服务发现的实现都采取了客户端模式。*



## 服务端发现模式

在服务端模式下，调用方直接向服务注册中心进行请求，服务注册中心再通过自身负载均衡策略，对微服务进行调用。这个模式下，调用方不需要在自身节点维护服务发现逻辑以及服务注册信息。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0555381e2d6c4e1e8067071e2746f68a~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

在服务端模式下： 1、 服务提供者向注册中心进行服务注册 2、 注册中心提供负载均衡功能， 3、 服务消费者去请求注册中心，由注册中心根据服务提供列表的健康情况，选择合适的服务提供者供服务消费者调用

现代容器化部署平台（如Docker和Kubernetes）就是服务端服务发现模式的一个例子，这些部署平台都具有内置的服务注册表和服务发现机制。容器化部署平台为每个服务提供路由请求的能力。服务客户端向路由器（或者负载均衡器）发出请求，容器化部署平台自动将请求路由到目标服务一个可用的服务实例。因此，**服务注册，服务发现和请求路由完全由容器化部署平台处理**。

》区别就是请求路由，负载均衡是由谁在提供

服务端发现模式的特点如下：

- 优点：
  - 服务消费者不需要关心服务提供者的列表，以及其采取何种负载均衡策略
  - 负载均衡策略的改变，只需要注册中心修改就行，不会出现新老算法同时存在的现象（注册中心统一改，而由客户端把握的情况下没法统一改）
  - 服务提供者上下线，对于服务消费者来说无感知
- 缺点：
  - 注册中心成为瓶颈，所有的请求都要经过注册中心，如果注册服务过多，服务消费者流量过大，可能会导致注册中心不可用
  - 微服务的一个目标是故障隔离，将整个系统切割为多个服务共同运行，如果某服务无法正常运行，只会影响到整个系统的相关部分功能，其它功能能够正常运行，即**去中心化**。然而，服务端发现模式实际上是集中式的做法，如果路由器或者负载均衡器无法提供服务，那么将导致整个系统瘫痪。



## 实现方案

### 1；file + 缓存

以文件的形式实现服务发现，这是一个比较简单的方案。其基本原理就是将服务提供者的信息(ip:port)写入文件中，服务消费者加载该文件，获取服务提供者的信息，根据一定的策略，进行访问。

需要注意的是，因为以文件形式提供服务发现，服务消费者要定期的去访问该文件，以获得最新的服务提供者列表，这里有个小优化点，就是可以有个线程定时去做该任务，首先去用该文件的最后一次修改时间跟服务上一次读取文件时候存储的修改时间做对比，如果时间一致，表明文件未做修改，那么就不需要重新做加载了，反之，重新加载文件。

### 2；zk

> ZooKeeper 是一个集中式服务，用于维护配置信息、命名、提供分布式同步和提供组服务。

zookeeper是一个树形结构，如上图所示。

使用zookeeper实现服务发现的功能，简单来讲，就是使用zookeeper作为注册中心。服务提供者在启动的时候，向zookeeper注册其信息，这个注册过程其实就是实际上在zookeeper中创建了一个znode节点，该节点存储了ip以及端口等信息，服务消费者向zookeeper获取服务提供者的信息。 服务注册、发现过程简述如下：

- 服务提供者启动时，会将其服务名称，ip地址注册到配置中心
- 服务消费者在第一次调用服务时，会通过注册中心找到相应的服务的IP地址列表，并缓存到本地，以供后续使用。当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从IP列表中取一个服务提供者的服务器调用服务
- 当服务提供者的某台服务器宕机或下线时，相应的ip会从服务提供者IP列表中移除。同时，注册中心会将新的服务IP地址列表发送给服务消费者机器，缓存在消费者本机
- 当某个服务的所有服务器都下线了，那么这个服务也就下线了
- 同样，当服务提供者的某台服务器上线时，注册中心会将新的服务IP地址列表发送给服务消费者机器，缓存在消费者本机
- 服务提供方可以根据服务消费者的数量来作为服务下线的依据



#### 服务注册

假设我们服务提供者的服务名称为services,首先在zookeeper上创建一个path /services,在服务提供者启动时候，向zookeeper进行注册，其注册的原理就是创建一个路径，路径为/services/$ip:port，其中ip:port为服务提供者实例的ip和端口。如下图所示，我们现在services实例有三个，其ip:port分别为192.168.1.1:1234、192.168.1.2:1234、192.168.1.3:1234和192.168.1.4:1234，如下图所示： ![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c9f7119b17f349e8a5220d8c690878c2~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

###### 健康检查

zookeeper实现了一种TTL的机制，就是如果客户端在一定时间内没有向注册中心发送心跳，则会将这个客户端摘除。

###### 获取服务提供者的列表

前面有提过，zookeeper实际上是一个树形结构，那么服务消费者是如何获取到服务提供者的信息呢？最重要的也是必须的一点就是 ***知道服务提供者信息的父节点路径***。以上图为例，我们需要知道

```bash
/services
复制代码
```

通过zookeeper client提供的接口 getchildren(path)来获取所有的子节点。

###### 感知服务上线与下线

zookeeper提供了“心跳检测”功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如192.168.1.2这台机器如果宕机了，那么zookeeper上的路径/services/下就会只剩下192.168.1.1:1234, 192.168.1.2:1234,192.168.1.4:1234。如下图所示：

![服务下线](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/64fc8c9aa85d44299b21117288a171cd~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

假设此时，重新上线一个实例，其ip为192.168.1.5，那么此时zookeeper树形结构如下图所示：

![服务上线](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d659b416f8e94e438adf23b0569420ac~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

