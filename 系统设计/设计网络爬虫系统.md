# 设计网络爬虫系统

## 什么是网络爬虫

网络爬虫（也称为蜘蛛）是一种用于下载，存储和分析网页的系统。它执行组织网页的任务，使得用户可以轻松找到信息。

## 用途

1. 搜索引擎使用网络爬虫收集网页并且生成索引，例如Google使用Googlebot网络爬虫。
2. 网络存档。可以使用网络爬虫来收集基于网络的信息并且将其存储起来以备将来使用。
3. 网络监控。网络爬虫可以监控互联网的版权和商标侵权行为。例如使用Digimarc使用爬虫来识别和报告盗版活动。

## 工作流程

一个简单的网络爬虫设计应该具有以下功能

- 给定一组url，访问url并且存储网页
- 提取网页中的url
- 将提取的新url附加到要访问的url列表中
- 递归这个过程

## 系统要求

1. 系统可拓展，由于太多的网页需要抓取和分析，网络爬虫应该通过使用并行化来有效地处理这个任务。
2. 爬虫应该是健壮的，以应对各种挑战，如格式不正确的html，无响应的服务器，崩溃和恶意链接。
3. 避免在短时间间隔向网站发出过多请求，导致DDos攻击。
4. 系统可拓展性，以便它可以灵活地应对未来的任何变化。例如，将来可能需要爬取图像或音乐文件的能力。
5. 监控，比如主机和页面的统计数据，爬虫速度性能，数据集的大小。

## 系统组件

1. 种子url，向网络爬虫提供一组种子地址，比如使用网站的域名来抓取其所有网页
2. url frontier，存储要下载的url的组件称为url frontier。爬网的一种方法是使用广度优化遍历，从种子url开始，通过将url frontier用户fifo队列实现，其中url将按照它们添加到队列中的顺序进行处理。
3. html fetcher，检索分析网页的实际内容
4. dns解析器，下载网页之前，必须将url转换为ip地址。
5. html解析器，解析，分析，验证内容的完整性和安全性，检查格式不正确的html或者可能导致存储系统出现问题的恶意软件
6. 重复检测，通过md5散列+哈希表/布隆过滤器识别重复，缓存提高性能
7.  数据存储，存储在大型分布式数据库中，例如hdfs
8. url extractor：从html页面中解析提取链接，添加到url frontier



